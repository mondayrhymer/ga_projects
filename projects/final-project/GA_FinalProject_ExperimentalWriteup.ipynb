{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melody Generator Project ##\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement & Goal**\n",
    "\n",
    "The goal of this project is to generate a sequence of notes of some set amount (a MIDI file) after some user input of a sequence of notes.  The total sequence of notes that is inputted plus generated will be called a melodic phrase.  Thus, a good model will generate the corresponding ending of the melodic phrase in the same key & register as the user inputted beginning portion of the phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset & Source**\n",
    "\n",
    "The data will take the form of Note Sequences called the MIDI file format, instead of Raw Audio.  This is because Raw Audio's data takes the form of waveforms, which by their very nature of being composed of frequencies & amplitudes, increases the size & complexity of this project.  As an alternative, MIDI encodes notes directly and can thus take the form of text, which allows notes to be mapped to numbers and is a lot more tractable.  \n",
    "\n",
    "Furthermore, MIDI can be imported into a music-based production software such as Apple's free GarageBand (or paid version, Logic Pro X), which would allow us to create & edit melodies using a MIDI control-like interface that is popular in the music industry.  \n",
    "\n",
    "Additionally, our data will inherently be considered time-series as opposed to cross-sectional, as notes take the form of a sequence.\n",
    "\n",
    "Training data will take the form of around 100 MIDI files *(still under consideration...number grabbed from looking at multiple tutorials and the size of their data sets and time taken to train)*  Songs in MIDI-format can be found for free here:  http://www.midiworld.com/files/\n",
    "\n",
    "A tentative list of python libraries to use:\n",
    "- pandas: for data analysis\n",
    "- py-midi: to work with midi files\n",
    "- tqdm: might need a progress bar\n",
    "- SciKit-Learn / Tensorflow: for model\n",
    "\n",
    "...and maybe?\n",
    "- msgpack-python: encode strings to binary\n",
    "- glob2: a better glob module that helps with matching filenames\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods & Models**\n",
    "\n",
    "*(Still under consideration)*\n",
    "\n",
    "Our model will take the form of a specific neural network called a Restricted Boltzmann Machine (RBM).  This would be a nice model to use since these can be trained in either supervised or unsupervised ways, and can be used for classification tasks.\n",
    "\n",
    "Other proposed models are LSTM and RNN, but may need to adjust depending upon tractability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics of Success**\n",
    "\n",
    "Since a melodic phrase consists of mainy different musical properties, for tractability concerns, we will have to limit ourselves to just two for this project.  We will concern ourselves with judging the model's generated portion of the melodic phrase by its ability to predict & match the correct key and register of the input.  The other components of a melody such as duration, rhythm, & pitch are currently left for further extension of this project.  We will also entertain the possibility of excluding the concept of harmony, as that includes multiple notes being played at once. *(Still under consideration)*  \n",
    "\n",
    "In musical theory, the \"key\" refers to the group of pitches/notes that forms the basis of a piece.  We will concern ourselves with melodic phrases that stay within the same key or group of notes, so that our model only needs to predict one key.\n",
    "\n",
    "Additionally, the \"register\" refers to the range of possible notes that are played in a piece.  Our model should extend or shorten the predicted register depending upon user input.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions, Assumptions, & Risks**\n",
    "\n",
    "Since a Melody is composed of two fundamental components, pitch & rhythm, it might be necessary to include rhythm in order to make the melodies comprehensible to the human ear (for instance, sound musical rather than robotic).  We might have to incorporate a note mapping scheme for a \"blank note\" (a pause or rest in music) which might allow us to incorporate a concept of rhythm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Audience & Impact**\n",
    "\n",
    "The results of this project are mainly intended for personal enjoyment.  However, possible implications and extensions of this line of work can lead to advances in generative music content.  This can either supplement current artists' workflow or perhaps lower costs for music production companies by automating content creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspiration & Sources**\n",
    "\n",
    "This project is inspired by the open-sourced Google Brain Project - Magenta, which uses the Tensorflow Library.  Since that project uses a neural net to deal with Raw Audio file formats, a simpler model was created to mimic the results for a much simpler data format, MIDI, which encodes musical notes directly.\n",
    "\n",
    "-----\n",
    "\n",
    "A tutorial of the aforementioned simpler model can be found here:  \n",
    "\n",
    "https://towardsdatascience.com/deep-learning-with-tensorflow-part-3-music-and-text-generation-8a3fbfdc5e9b\n",
    "\n",
    "Repo found here:\n",
    "\n",
    "https://github.com/burliEnterprises/tensorflow-music-generator\n",
    "\n",
    "-----\n",
    "\n",
    "A Jupyter Notebook of using the NSynth model from Project Magenta can be found here:\n",
    "\n",
    "https://github.com/tensorflow/magenta-demos/blob/master/jupyter-notebooks/NSynth.ipynb\n",
    "\n",
    "...with a tutorial for generating your own sounds with NSynth here:\n",
    "\n",
    "https://magenta.tensorflow.org/nsynth-fastgen\n",
    "\n",
    "...and the whole Project Magenta repo here:\n",
    "\n",
    "https://github.com/tensorflow/magenta\n",
    "\n",
    "-----\n",
    "\n",
    "A similar concept by Project Magenta, but uses the MIDI format can be found in the AI duet here (only works in Python 2.7!):\n",
    "\n",
    "https://experiments.withgoogle.com/ai/ai-duet\n",
    "\n",
    "https://github.com/googlecreativelab/aiexperiments-ai-duet\n",
    "\n",
    "-----\n",
    "\n",
    "Similar Character-Based Model using LSTM here:\n",
    "\n",
    "https://github.com/IraKorshunova/folk-rnn\n",
    "\n",
    "-----\n",
    "\n",
    "Using an RNN here (allows temporality to get rhythms):\n",
    "\n",
    "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/\n",
    "\n",
    "-----\n",
    "\n",
    "A compilation of Deep Learning Tools for Music Generation:\n",
    "\n",
    "http://www.asimovinstitute.org/analyzing-deep-learning-tools-music/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
